import pandas as pd
import numpy as np
import os
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from src.utils.data_processing import load_and_preprocess_data

def filter_residential_data(df):
    """
    Filtra apenas dados residenciais (Apartamento e Casa).
    
    JUSTIFICATIVA: ExcluÃ­mos imÃ³veis comerciais/institucionais para evitar:
    - Hospitais, shoppings e prÃ©dios comerciais gigantes
    - Terrenos vazios com valores distorcidos  
    - Salas comerciais com padrÃµes de preÃ§o diferentes
    - Garantir homogeneidade na anÃ¡lise residencial
    """
    print("=== FILTRAGEM PARA DADOS RESIDENCIAIS ===")
    
    residential_types = ['Apartamento', 'Casa']
    df_residential = df[df['tipo_imovel'].isin(residential_types)].copy()
    
    print(f"Dataset original: {len(df):,} registros")
    print(f"Dataset filtrado: {len(df_residential):,} registros")
    reduction = (1 - len(df_residential)/len(df)) * 100
    print(f"ReduÃ§Ã£o: {reduction:.1f}% (foco em dados residenciais)")
    
    print(f"\nTipos incluÃ­dos:")
    for tipo in residential_types:
        count = (df_residential['tipo_imovel'] == tipo).sum()
        pct = (count / len(df_residential)) * 100
        print(f"  â€¢ {tipo}: {count:,} ({pct:.1f}%)")
    
    return df_residential

def prepare_clustering_features(df_residential):
    """
    Prepara features para clusterizaÃ§Ã£o focando em caracterÃ­sticas residenciais.
    
    ATENÃ‡ÃƒO: 'valor_m2' foi removido das features de clusterizaÃ§Ã£o para que os clusters
    refletem o padrÃ£o construtivo e nÃ£o o efeito de mercado (preÃ§o/localizaÃ§Ã£o).
    'padrao_acabamento' Ã© incluÃ­do via One-Hot Encoding.
    """
    print(f"\n=== PREPARAÃ‡ÃƒO DAS FEATURES ===")
    
    # Features numÃ©ricas relevantes para residÃªncias
    numerical_features = ['area_construida', 'area_terreno', 'ano_construcao']
    
    # Feature categÃ³rica a ser usada e encodificada
    categorical_features = ['padrao_acabamento']
    
    # Colunas para manter no DataFrame final para anÃ¡lise (mesmo que nÃ£o usadas para clustering)
    analysis_cols = ['bairro', 'tipo_imovel', 'valor_m2']
    
    # Criar DataFrame com todas as features necessÃ¡rias para o processo
    df_features = df_residential[numerical_features + categorical_features + analysis_cols].copy()
    
    # Remover outliers extremos (alÃ©m do 99Âº percentil) para as features numÃ©ricas
    for feature in numerical_features:
        q99 = df_features[feature].quantile(0.99)
        q01 = df_features[feature].quantile(0.01)
        df_features = df_features[
            (df_features[feature] >= q01) & (df_features[feature] <= q99)
        ]
    
    # Aplicar One-Hot Encoding para 'padrao_acabamento'
    df_features = pd.get_dummies(df_features, columns=categorical_features, prefix=categorical_features, drop_first=False)
    
    # Atualizar a lista de features para incluir as colunas one-hot encoded
    # As novas colunas terÃ£o o formato 'padrao_acabamento_VALOR'.
    # Primeiro, identificamos as colunas criadas pelo get_dummies
    encoded_feature_names = [col for col in df_features.columns if col.startswith('padrao_acabamento_')]
    
    # A lista final de features para clustering serÃ¡ numÃ©rica + encoded
    final_clustering_features = numerical_features + encoded_feature_names
    
    print(f"ApÃ³s remoÃ§Ã£o de outliers e encoding: {len(df_features):,} registros")
    print(f"Features selecionadas para clusterizaÃ§Ã£o (incluindo One-Hot Encoding): {final_clustering_features}")
    
    return df_features, final_clustering_features

def perform_clustering(df_features, features, n_clusters=5):
    """
    Executa clusterizaÃ§Ã£o K-means nos dados residenciais.
    """
    print(f"\n=== CLUSTERIZAÃ‡ÃƒO K-MEANS ===")
    
    # Preparar dados para clustering
    X = df_features[features].values
    
    # Normalizar features
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    # Executar K-means
    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
    clusters = kmeans.fit_predict(X_scaled)
    
    # Calcular score de silhueta
    silhouette = silhouette_score(X_scaled, clusters)
    
    # Adicionar clusters ao DataFrame
    df_clustered = df_features.copy()
    df_clustered['cluster'] = clusters
    
    print(f"NÃºmero de clusters: {n_clusters}")
    print(f"Silhouette Score: {silhouette:.3f}")
    print(f"Registros clusterizados: {len(df_clustered):,}")
    
    # Analisar clusters
    print(f"\nDistribuiÃ§Ã£o dos clusters:")
    cluster_counts = pd.Series(clusters).value_counts().sort_index()
    for cluster_id, count in cluster_counts.items():
        pct = (count / len(clusters)) * 100
        print(f"  Cluster {cluster_id}: {count:,} ({pct:.1f}%)")
    
    return df_clustered, kmeans, scaler, silhouette

def analyze_clusters(df_clustered, features):
    """
    Analisa caracterÃ­sticas de cada cluster.
    """
    print(f"\n=== ANÃLISE DOS CLUSTERS ===")
    
    # As features para o summary devem ser as originais numÃ©ricas e o valor_m2
    # Nomes das colunas one-hot encoded nÃ£o sÃ£o ideais para o summary 'mean'/'median'
    summary_features_for_agg = [col for col in df_clustered.columns if col in ['area_construida', 'area_terreno', 'ano_construcao', 'valor_m2']]
    cluster_summary = df_clustered.groupby('cluster')[summary_features_for_agg].agg(['mean', 'median']).round(2)
    
    for cluster_id in sorted(df_clustered['cluster'].unique()):
        print(f"\nCluster {cluster_id}:")
        cluster_data = df_clustered[df_clustered['cluster'] == cluster_id]
        
        # CaracterÃ­sticas principais
        valor_m2_med = cluster_data['valor_m2'].median()
        area_med = cluster_data['area_construida'].median()
        ano_med = cluster_data['ano_construcao'].median()
        
        print(f"  â€¢ Valor mÂ²: R$ {valor_m2_med:,.0f} (mediana)")
        print(f"  â€¢ Ãrea construÃ­da: {area_med:.0f} mÂ² (mediana)")
        print(f"  â€¢ Ano construÃ§Ã£o: {ano_med:.0f} (mediana)")
        
        # Tipo de imÃ³vel predominante
        tipo_predominante = cluster_data['tipo_imovel'].value_counts().index[0]
        pct_tipo = (cluster_data['tipo_imovel'].value_counts().iloc[0] / len(cluster_data)) * 100
        print(f"  â€¢ Tipo predominante: {tipo_predominante} ({pct_tipo:.1f}%)")
        
        # PadrÃ£o de acabamento predominante
        if 'padrao_acabamento' in cluster_data.columns:
            acabamento_predominante = cluster_data['padrao_acabamento'].value_counts().index[0]
            pct_acabamento = (cluster_data['padrao_acabamento'].value_counts().iloc[0] / len(cluster_data)) * 100
            print(f"  â€¢ PadrÃ£o Acabamento predominante: {acabamento_predominante} ({pct_acabamento:.1f}%)")
        
        # Bairros mais comuns
        top_bairros = cluster_data['bairro'].value_counts().head(3)
        print(f"  â€¢ Bairros principais: {', '.join(top_bairros.index[:3])}")
    
    return cluster_summary

def create_cluster_visualizations(df_clustered):
    """
    Cria visualizaÃ§Ãµes dos clusters.
    """
    print(f"\n=== CRIANDO VISUALIZAÃ‡Ã•ES ===")
    
    # GrÃ¡fico 1: Scatter plot Valor mÂ² vs Ãrea construÃ­da
    fig1 = px.scatter(
        df_clustered, 
        x='area_construida', 
        y='valor_m2',
        color='cluster',
        hover_data=['bairro', 'tipo_imovel', 'ano_construcao', 'padrao_acabamento'],
        title='Clusters: Valor mÂ² vs Ãrea ConstruÃ­da (Dados Residenciais)',
        labels={
            'area_construida': 'Ãrea ConstruÃ­da (mÂ²)',
            'valor_m2': 'Valor por mÂ² (R$)',
            'cluster': 'Cluster'
        }
    )
    
    # GrÃ¡fico 2: Box plot do valor mÂ² por cluster
    fig2 = px.box(
        df_clustered,
        x='cluster',
        y='valor_m2',
        title='DistribuiÃ§Ã£o do Valor mÂ² por Cluster (Dados Residenciais)',
        labels={
            'cluster': 'Cluster',
            'valor_m2': 'Valor por mÂ² (R$)'
        }
    )
    
    # GrÃ¡fico 3: Contagem por tipo de imÃ³vel e cluster
    cluster_type_counts = df_clustered.groupby(['cluster', 'tipo_imovel']).size().reset_index(name='count')
    fig3 = px.bar(
        cluster_type_counts,
        x='cluster',
        y='count',
        color='tipo_imovel',
        title='DistribuiÃ§Ã£o de Tipos de ImÃ³veis por Cluster',
        labels={
            'cluster': 'Cluster',
            'count': 'NÃºmero de ImÃ³veis',
            'tipo_imovel': 'Tipo de ImÃ³vel'
        }
    )

    # GrÃ¡fico 4: Contagem por PadrÃ£o de Acabamento e Cluster
    if 'padrao_acabamento' in df_clustered.columns:
        cluster_acabamento_counts = df_clustered.groupby(['cluster', 'padrao_acabamento']).size().reset_index(name='count')
        fig4 = px.bar(
            cluster_acabamento_counts,
            x='cluster',
            y='count',
            color='padrao_acabamento',
            title='DistribuiÃ§Ã£o de PadrÃ£o de Acabamento por Cluster',
            labels={
                'cluster': 'Cluster',
                'count': 'NÃºmero de ImÃ³veis',
                'padrao_acabamento': 'PadrÃ£o de Acabamento'
            }
        )
        return fig1, fig2, fig3, fig4
    else:
        return fig1, fig2, fig3

def main():
    """
    FunÃ§Ã£o principal que executa toda a anÃ¡lise de clusterizaÃ§Ã£o.
    """
    print("ğŸ  CLUSTERIZAÃ‡ÃƒO DE DADOS RESIDENCIAIS - ITBI RECIFE")
    print("=" * 60)
    
    # 1. Carregar dados
    print("1. Carregando dados...")
    df = load_and_preprocess_data()
    
    # 2. Filtrar dados residenciais
    print(f"\n2. Filtrando dados residenciais...")
    df_residential = filter_residential_data(df)
    
    # 3. Preparar features
    print(f"\n3. Preparando features...")
    df_features, features = prepare_clustering_features(df_residential)
    
    # 4. Executar clusterizaÃ§Ã£o
    print(f"\n4. Executando clusterizaÃ§Ã£o...")
    df_clustered, kmeans, scaler, silhouette = perform_clustering(df_features, features)
    
    # 5. Analisar clusters
    print(f"\n5. Analisando clusters...")
    cluster_summary = analyze_clusters(df_clustered, features)
    
    # 6. Criar visualizaÃ§Ãµes
    print(f"\n6. Criando visualizaÃ§Ãµes...")
    figures = create_cluster_visualizations(df_clustered)
    
    print(f"\nâœ… AnÃ¡lise de clusterizaÃ§Ã£o concluÃ­da!")
    print(f"ğŸ“Š {len(df_clustered):,} imÃ³veis residenciais clusterizados")
    print(f"ğŸ¯ Silhouette Score: {silhouette:.3f}")
    
    return df_clustered, figures, cluster_summary

def get_cache_paths():
    script_dir = os.path.dirname(os.path.abspath(__file__))
    cache_dir = os.path.join(script_dir, 'data') # Agora aponta para PISI3-Project/data
    os.makedirs(cache_dir, exist_ok=True)
    cache_file = os.path.join(cache_dir, 'clustering_cache.parquet')
    metadata_file = os.path.join(cache_dir, 'clustering_metadata.json')
    return cache_file, metadata_file, cache_dir

def save_clustering_cache(df_clustered, silhouette_score, features):
    """
    Salva os resultados da clusterizaÃ§Ã£o em cache para carregamento rÃ¡pido.
    """
    cache_file, metadata_file, _ = get_cache_paths()
    
    # Salvar DataFrame com clusters
    df_clustered.to_parquet(cache_file, engine='pyarrow', compression='snappy')
    
    # Salvar metadados em arquivo separado
    metadata = {
        'silhouette_score': silhouette_score,
        'features': features,
        'n_clusters': 5,
        'total_records': len(df_clustered)
    }
    
    import json
    with open(metadata_file, 'w') as f:
        json.dump(metadata, f)
    
    print(f"âœ… Cache da clusterizaÃ§Ã£o salvo:")
    print(f"   â€¢ Dados: {cache_file}")
    print(f"   â€¢ Metadata: {metadata_file}")
    print(f"   â€¢ Registros: {len(df_clustered):,}")

def load_clustering_cache():
    """
    Carrega os resultados da clusterizaÃ§Ã£o do cache se disponÃ­vel.
    
    Returns:
        tuple: (df_clustered, silhouette_score, features) ou None se cache nÃ£o existir
    """
    cache_file, metadata_file, _ = get_cache_paths()
    
    if not os.path.exists(cache_file) or not os.path.exists(metadata_file):
        return None
    
    try:
        # Carregar dados
        df_clustered = pd.read_parquet(cache_file, engine='pyarrow')
        
        # Carregar metadata
        import json
        with open(metadata_file, 'r') as f:
            metadata = json.load(f)
        
        silhouette_score = metadata['silhouette_score']
        features = metadata['features']
        
        print(f"âœ… Cache da clusterizaÃ§Ã£o carregado:")
        print(f"   â€¢ Registros: {len(df_clustered):,}")
        print(f"   â€¢ Silhouette Score: {silhouette_score:.3f}")
        
        return df_clustered, silhouette_score, features
        
    except Exception as e:
        print(f"âŒ Erro ao carregar cache: {e}")
        return None

def get_clustering_data_optimized():
    """
    FunÃ§Ã£o otimizada que usa cache quando possÃ­vel, senÃ£o processa do zero.
    """
    print("ğŸš€ Carregando dados de clusterizaÃ§Ã£o...")
    
    # Tentar carregar do cache primeiro
    cached_result = load_clustering_cache()
    
    if cached_result is not None:
        print("âš¡ Dados carregados do cache (ultra-rÃ¡pido)!")
        # O cache pode nÃ£o ter a coluna 'padrao_acabamento' one-hot encoded se foi salvo antes da mudanÃ§a
        # EntÃ£o, precisamos verificar e recriar as features para o K-Means, se necessÃ¡rio.
        # Para simplificar agora, vou forÃ§ar o reprocessamento se as features mudaram muito.
        # Em um sistema real, vocÃª teria uma lÃ³gica de versÃ£o para o cache.
        df_clustered, silhouette, cached_features_list = cached_result
        
        # Verificar se as features do cache correspondem Ã s features esperadas apÃ³s a atualizaÃ§Ã£o
        # Esta Ã© uma verificaÃ§Ã£o simplificada; uma abordagem robusta compararia conjuntos de features
        expected_numerical_features = ['area_construida', 'area_terreno', 'ano_construcao']
        # VerificaÃ§Ã£o se o cache foi gerado *antes* do one-hot encoding de padrao_acabamento
        if not any(f.startswith('padrao_acabamento_') for f in cached_features_list):
            print("âš ï¸ Cache antigo detectado (sem One-Hot Encoding de padrÃ£o de acabamento). Reprocessando...")
            # ForÃ§a o reprocessamento para garantir que o One-Hot Encoding seja aplicado
            return process_and_save_new_clustering_data()
        
        # Se o cache Ã© vÃ¡lido e jÃ¡ tem as features corretas, retorna-o
        return df_clustered, silhouette, cached_features_list
    
    print("ğŸ”„ Cache nÃ£o encontrado ou invÃ¡lido, processando dados...")
    return process_and_save_new_clustering_data()

def process_and_save_new_clustering_data():
    """
    Processa os dados de clusterizaÃ§Ã£o do zero e salva no cache.
    """
    df = load_and_preprocess_data()
    df_residential = filter_residential_data(df)
    df_features, features = prepare_clustering_features(df_residential)
    df_clustered, kmeans, scaler, silhouette = perform_clustering(df_features, features)
    
    # Salvar no cache para prÃ³ximas vezes
    save_clustering_cache(df_clustered, silhouette, features)
    
    return df_clustered, silhouette, features

if __name__ == "__main__":
    # Modificando a chamada para main para refletir as novas features retornadas
    df_clustered, figures, summary = main()

    # Exibir as figuras se o ambiente permitir (apenas para teste local)
    # if len(figures) == 4:
    #     fig1, fig2, fig3, fig4 = figures
    #     fig1.show()
    #     fig2.show()
    #     fig3.show()
    #     fig4.show()
    # elif len(figures) == 3:
    #     fig1, fig2, fig3 = figures
    #     fig1.show()
    #     fig2.show()
    #     fig3.show()